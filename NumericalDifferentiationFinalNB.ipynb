{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NumericalDifferentiationFinalNotebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Stj9oH-nISD"
      },
      "source": [
        "## **Numerical Differentiation in Coding: The Pythonic Way**\r\n",
        "Have you had **problems coding** the differential value of a function `f(x)`? Do you need a **functional approach** that can automate differentiation for you? If the answer to either of these queries is a yes, then this blog post is definitely meant for you.  \r\n",
        "\r\n",
        "\r\n",
        "![](https://cdn-images-1.medium.com/max/1100/1*jCO4wnwzF9ISWOmIclrkLQ.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekuT0LuNoytE"
      },
      "source": [
        "### **Why Numerical Differentiation?**\r\n",
        "Consider the following problem :\r\n",
        "\r\n",
        "$$\r\n",
        "\\begin{aligned}\r\n",
        "&\\text { Differentiate the following function with respect to x: }\\\\\r\n",
        "&\\sin ^{-1}\\left\\{\\sqrt{1-x^{2}}\\right\\}, 0<x<1\r\n",
        "\\end{aligned}\r\n",
        "$$\r\n",
        "\r\n",
        "Before writing some formal code, import the math module in Python:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeY_HfFdmIcO"
      },
      "source": [
        "import math"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuWhwUum9-mv"
      },
      "source": [
        "Now, you might construct the objective function as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emjVyo2t997n"
      },
      "source": [
        "def f(x):\r\n",
        "  return math.asin(math.sqrt(1-math.pow(x,2)))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKx-7_pD-Fvz"
      },
      "source": [
        "For the differential function `f'(x)`, a Typical Mathematician would thereafter go on to manually differentiate the function using lengthy and complex chain rules, that might look as follows and then code it:\r\n",
        "\r\n",
        "$$\r\n",
        "\\begin{array}{c}\r\n",
        "\\frac{\\mathrm{d}}{\\mathrm{d} x}\\left[\\arcsin \\left(\\sqrt{1-x^{2}}\\right)\\right] \\\\\r\n",
        "=\\frac{1}{\\sqrt{1-\\left(\\sqrt{1-x^{2}}\\right)^{2}}} \\cdot \\frac{\\mathrm{d}}{\\mathrm{d} x}\\left[\\sqrt{1-x^{2}}\\right] \\\\\r\n",
        "=\\frac{\\frac{\\mathrm{d}}{\\mathrm{d} x}[1]-\\frac{\\mathrm{d}}{\\mathrm{d} x}\\left[x^{2}\\right]}{2 \\sqrt{1-x^{2}}|x|} \\\\\r\n",
        "=\\frac{0-2 x}{2 \\sqrt{1-x^{2}}|x|} \\\\\r\n",
        "=-\\frac{x}{\\sqrt{1-x^{2}}|x|}\r\n",
        "\\end{array}\r\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDbSsZd1-Coz"
      },
      "source": [
        "def differentialfx(x):\r\n",
        "  return (-x)/(abs(x) * math.sqrt(1-math.pow(x,2)))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WARI1Ghl-Voc"
      },
      "source": [
        "Scary, eh? Well, we're not here for that, we will try to automate the **differentiation operator**(`d/dx`) as a whole.\r\n",
        "\r\n",
        "![](https://cdn-images-1.medium.com/max/1100/1*HNx9EifyMJbdFVCUqKJOzQ.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBWWEq1L-mRt"
      },
      "source": [
        "### **Concept of Limits, Derivatives, and Approximations**\r\n",
        "The limit of a function at a point a in its domain (if it exists) is the value that the function approaches as its argument approaches a.\r\n",
        "\r\n",
        "$$\r\n",
        "\\lim _{x \\rightarrow a} f(x)=L\r\n",
        "$$\r\n",
        "\r\n",
        "We will use the above definition to obtain the Left & Right Hand Derivatives, respectively as follows:\r\n",
        "\r\n",
        "The left-hand and right-hand derivatives of $f$ at $a$ are defined by\r\n",
        "$$\r\n",
        "f_{-}^{\\prime}(a)=\\lim _{h \\rightarrow 0^{-}} \\frac{f(a+h)-f(a)}{h} = \\lim _{h \\rightarrow 0^{+}} \\frac{f(a-h)-f(a)}{-h} = \\lim _{h \\rightarrow 0^{+}} \\frac{f(a)-f(a-h)}{h}\r\n",
        "$$\r\n",
        "and\r\n",
        "$$\r\n",
        "f_{+}^{\\prime}(a)=\\lim _{h \\rightarrow 0^{+}} \\frac{f(a+h)-f(a)}{h}\r\n",
        "$$\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "For better approximation often the average of the two is considered the best estimate of the differential at x=a. Mathematically, what we are trying to compute is:\r\n",
        "\r\n",
        "$$\r\n",
        "f^{\\prime}(x= a) = \\frac{f_{+}^{\\prime}(a)+ f_{-}^{\\prime}(a)}{2} = \\lim _{h \\rightarrow 0^{+}} \\frac{f(a+h)-f(a-h)}{2h}\r\n",
        "$$\r\n",
        "\r\n",
        "Hence to get the differential value of f(x) at x=a [`f'(x=a)`] we will use the above formula, substituting a very small positive number {say $10^{-6}$ } for h. Let's dive right into coding it :\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6D8-yYo8-U5r"
      },
      "source": [
        "def differentiate(func,a,h=1e-6):\r\n",
        "  \"\"\"\r\n",
        "  Returns a derivative of the passed function f(x) at a given value\r\n",
        "  a using the concept of Right Hand and Left Hand derivative\r\n",
        "  approximation.\r\n",
        "  f = univariate function f(x)\r\n",
        "  a = value at which derivative should be calculated f'(x=a)\r\n",
        "  h = the tolerance h, which is a value very close to zero but not  \r\n",
        "      zero.\r\n",
        "  \"\"\"\r\n",
        "  return (func(a+h)-func(a-h))/(2*h)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxXSEuW4_Uou"
      },
      "source": [
        "Results with Verification\r\n",
        "Now let us find out some differentials using the above-defined scripts for `0<x<1`. The approach we will use here is simple :\r\n",
        "- **Loop** through `num` from 0.01 to 0.99\r\n",
        "- **Calculate differential** using hard-coded method & numerical differentiation\r\n",
        "- Know if the value of the two computed values are **close to each other** or not.\r\n",
        "- If the values aren't close, **store and display** the value.\r\n",
        "- **Inspect the individual values** where the results were not close."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpNQ71z7_SaW",
        "outputId": "d749fec7-0daf-416e-d604-fff4a2f87a5f"
      },
      "source": [
        "DIFFS = []\r\n",
        "for i in range(1,100):\r\n",
        "  num = i/100\r\n",
        "  approx_estimated_value = differentiate(f,num)\r\n",
        "  actual_true_value = differentialfx(num)\r\n",
        "  val = math.isclose(approx_estimated_value, actual_true_value,rel_tol=1e-9)\r\n",
        "  if val == False:\r\n",
        "    print(\"Suitable Difference Found at\",num)\r\n",
        "    DIFFS.append(num)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Suitable Difference Found at 0.02\n",
            "Suitable Difference Found at 0.99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuvuko6T_xGY"
      },
      "source": [
        "Note: These differences are occurring because of approximation errors. If we wish to check till `n` digits after the decimal, these differences can then be avoided by using `n=6` (or a lower value) in `math.isclose(approx_estimated_value, actual_true_value,rel_tol=1e-n)`. In this case no drastic differences are found."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CZO-Co7_3vc"
      },
      "source": [
        "**Inspect the Differences :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJrRKeba_tic",
        "outputId": "15b26114-274c-49f7-eaeb-cf70635cbd5a"
      },
      "source": [
        "for each_num in DIFFS:\r\n",
        "  print(\"Values at\",each_num,\":\",differentiate(f,each_num,h=1e-6), \",\",differentialfx(each_num))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Values at 0.02 : -1.0002000616626816 , -1.000200060020007\n",
            "Values at 0.99 : -7.088812059172223 , -7.088812050083354\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YGJlTVz_-Mq"
      },
      "source": [
        "The values only differ after 8 digits or so, hence running the script with `math.isclose(approx_estimated_value, actual_true_value,rel_tol=1e-8)` shows that there are no differences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCDWSTAv_7aN"
      },
      "source": [
        "DIFFS = []\r\n",
        "for i in range(1,100):\r\n",
        "  num = i/100\r\n",
        "  approx_estimated_value = differentiate(f,num)\r\n",
        "  actual_true_value = differentialfx(num)\r\n",
        "  val = math.isclose(approx_estimated_value, actual_true_value,rel_tol=1e-8)\r\n",
        "  if val == False:\r\n",
        "    print(\"Suitable Difference Found at\",num)\r\n",
        "    DIFFS.append(num)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFTQKUwQAIT8"
      },
      "source": [
        "![](https://cdn-images-1.medium.com/max/1100/1*VbNJZ3rQlr5y6H9dyle6nw.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vR4-PoroANJ3"
      },
      "source": [
        "### **Higher Level Information**\r\n",
        "PyTorch (a Python deep learning module) has **AutoGrad** Features that employ Chain Rule Mechanisms of Differential Calculus using complex tree-like structures (graphs) that perform the same in a more efficient and faster way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdZ8ksHfADHr",
        "outputId": "a74c453a-4696-49f1-df56-3d0208524d8d"
      },
      "source": [
        "import torch\r\n",
        "x = torch.autograd.Variable(torch.Tensor([0.5]),requires_grad=True)\r\n",
        "def fnew(x):\r\n",
        "  return torch.asin(torch.sqrt(1-torch.pow(x,2)))\r\n",
        "y = fnew(x)\r\n",
        "y.backward()\r\n",
        "print(float(x.grad))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-1.154700517654419\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQZYBv-JAhI_"
      },
      "source": [
        "However, the implementation from the `math` module will not work whilst working with PyTorch. We have to use the `torch` module to use `autograd` features. Since this is a vast topic beyond the scope of this discussion, here are some of the ways via which you can learn how to implement Automated Differentiation using AutoGrad in PyTorch: \r\n",
        "- [PyTorch AutoGrad Official Tutorial/Documentation](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)\r\n",
        "- [Pytorch Tutorial - Gradient Descent with Autograd and Backpropagation by Python Engineer](https://www.youtube.com/watch?v=E-I2DNVzQLg)\r\n",
        "\r\n",
        "Note : The implementation mentioned in this blog is for displaying the easiest way of differentiation - the numerical way but since it requires another function bypass call it should not be used on a large scale because a suitable time loss will occur. The below section shows the suitable implementation time differences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYp73vTbBdaZ"
      },
      "source": [
        "### **Running Time of Scripts**\r\n",
        "After running the three methods separately and timing those scripts the following averaged timing data is obtained:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1dOz5XlAURT",
        "outputId": "da0c052d-9214-4c57-cf37-e506ec8fe279"
      },
      "source": [
        "def ClassicalDifferential(x):\r\n",
        "  return differentialfx(x)\r\n",
        "\r\n",
        "def NumericalDifferential(x):\r\n",
        "  return differentiate(f,x,h=1e-6)\r\n",
        "\r\n",
        "def TorchDifferential(x):\r\n",
        "  x = torch.autograd.Variable(torch.Tensor([0.5]),requires_grad=True)\r\n",
        "  y = fnew(x)\r\n",
        "  y.backward()\r\n",
        "  return float(x.grad)\r\n",
        "\r\n",
        "def RunDifferentiation(func):\r\n",
        "  DIFFS = []\r\n",
        "  for i in range(1,100):\r\n",
        "    num = i/100\r\n",
        "    diff = func(num)\r\n",
        "    DIFFS.append(diff)\r\n",
        "    \r\n",
        "def RunMultiple(func,n):\r\n",
        "  for i in range(n):\r\n",
        "    x= RunDifferentiation(func)\r\n",
        "  return 0\r\n",
        "\r\n",
        "import time\r\n",
        "\r\n",
        "def TimeFuncRun(func, n=100):\r\n",
        "  start = time.time()\r\n",
        "  ans = RunMultiple(func,n)\r\n",
        "  return (time.time()-start)/n\r\n",
        "\r\n",
        "n = 100\r\n",
        "print('Over ',n,' Iterations :')\r\n",
        "print('ClassicalDifferential :', TimeFuncRun(ClassicalDifferential, n), 'seconds.')\r\n",
        "print('NumericalDifferential :', TimeFuncRun(NumericalDifferential, n), 'seconds.')\r\n",
        "print('TorchDifferential :', TimeFuncRun(TorchDifferential, n), 'seconds.')\r\n",
        "\r\n",
        "n = 1000\r\n",
        "print('Over ',n,' Iterations :')\r\n",
        "print('ClassicalDifferential :', TimeFuncRun(ClassicalDifferential, n), 'seconds.')\r\n",
        "print('NumericalDifferential :', TimeFuncRun(NumericalDifferential, n), 'seconds.')\r\n",
        "print('TorchDifferential :', TimeFuncRun(TorchDifferential, n), 'seconds.')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Over  100  Iterations :\n",
            "ClassicalDifferential : 5.965709686279297e-05 seconds.\n",
            "NumericalDifferential : 0.00013366222381591797 seconds.\n",
            "TorchDifferential : 0.010680837631225586 seconds.\n",
            "Over  1000  Iterations :\n",
            "ClassicalDifferential : 6.014037132263184e-05 seconds.\n",
            "NumericalDifferential : 0.00012920022010803222 seconds.\n",
            "TorchDifferential : 0.010593115329742431 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS1cCXexB34k"
      },
      "source": [
        "### **Further Reading & Understanding**\r\n",
        "If any/all of these don't make complete sense to you, consider reading these articles to enhance your POC:\r\n",
        "- [Brilliant - Limits of Functions](https://brilliant.org/wiki/limits-of-functions/)\r\n",
        "- [CueMath - LHD & RHD](https://www.cuemath.com/jee/left-hand-and-right-hand-derivatives-limits-continuity-differentiability/)"
      ]
    }
  ]
}